{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dda983e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T09:15:20.031163Z",
     "start_time": "2022-04-05T09:15:20.024612Z"
    }
   },
   "outputs": [],
   "source": [
    "# 결과 확인을 용이하게 하기 위한 코드\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072c11b1",
   "metadata": {},
   "source": [
    "- [07 PyTorch에서 다양한 장치 간 모델을 저장하고 불러오기](https://tutorials.pytorch.kr/recipes/recipes/save_load_across_devices.html)\n",
    "- 다양한 장치(device)에서 당신의 신경망 모델을 저장하거나 불러오고 싶은 경우가 생길 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1003157f",
   "metadata": {},
   "source": [
    "# 개요\n",
    "- PyTorch를 사용하여 장치 간의 모델을 저장하거나 불러오는 것은 비교적 간단함\n",
    "- 이번 레시피에서는, CPU와 GPU에서 모델을 저장하고 불러오는 방법을 실험할 것임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3db77bd",
   "metadata": {},
   "source": [
    "# 설정\n",
    "- 이번 레시피에서 모든 코드 블록이 제대로 실행되게 하려면, 우선 런타임(runtime) 설정을 “GPU”나 더 높게 지정해주어야 함\n",
    "- 이후, 아래와 같이 ``torch``를 설치해야 PyTorch를 사용할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1fcef50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T09:26:14.080404Z",
     "start_time": "2022-04-05T09:26:14.077980Z"
    }
   },
   "outputs": [],
   "source": [
    "# pip install torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce6b8db",
   "metadata": {},
   "source": [
    "# 단계\n",
    "1. 데이터 활용에 필요한 모든 라이브러리 Import 하기\n",
    "2. 신경망을 구성하고 초기화하기\n",
    "3. GPU에서 저장하고 CPU에서 불러오기\n",
    "4. GPU에서 저장하고 GPU에서 불러오기\n",
    "5. CPU에서 저장하고 GPU에서 불러오기\n",
    "6. `DataParallel` 모델을 저장하고 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e2ec45",
   "metadata": {},
   "source": [
    "## 데이터 활용에 필요한 모든 라이브러리 Import 하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bb6b8ec",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T09:37:05.559366Z",
     "start_time": "2022-04-05T09:37:05.136322Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19804ef0",
   "metadata": {},
   "source": [
    "## 신경망을 구성하고 초기화하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1da45dd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T09:42:20.223272Z",
     "start_time": "2022-04-05T09:42:20.216139Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5) # in_channels, out_channels, kernel_size\n",
    "        self.pool = nn.MaxPool2d(2, 2) # kernel_size, stride\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120) # in_features, out_features, bias(default=True)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        \n",
    "        # fc1에 들어갈 in_features의 크기에 맞춰주는 듯\n",
    "        # -1인자는 아마도 배치 사이즈...?\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a7a32e",
   "metadata": {},
   "source": [
    "## GPU에서 저장하고 CPU에서 불러오기\n",
    "- GPU로 학습된 모델을 CPU에서 불러올 때는 `torch.load()` 함수의 `map_location` 인자에 `torch.device(‘cpu’)`를 전달합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b49039bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-05T09:44:43.260277Z",
     "start_time": "2022-04-05T09:44:43.240810Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 저장하고자 하는 경로를 지정함\n",
    "PATH = 'model.pt'\n",
    "\n",
    "# 저장하기\n",
    "torch.save(net.state_dict(), PATH)\n",
    "\n",
    "# 불러오기\n",
    "device = torch.device('cpu')\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load(PATH, map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4785c0",
   "metadata": {},
   "source": [
    "- 이 경우, Tensor의 저장된 내용은 `map_location` 인자를 통하여 CPU 장치에 동적으로 재배치됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1184a858",
   "metadata": {},
   "source": [
    "## GPU에서 저장하고 GPU에서 불러오기\n",
    "- GPU에서 학습 및 저장된 모델을 GPU에서 불러올 때는 초기화된 모델에 `model.to(torch.device('cuda'))`를 호출하여 CUDA에 최적화된 모델로 변환해주기\n",
    "- 그리고 모든 입력에 `.to(torch.device('cuda'))` 함수를 호출해야 모델에 데이터를 제공할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b8c630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장하기\n",
    "torch.save(net.state_dict(), PATH)\n",
    "\n",
    "# 불러오기\n",
    "device = torch.device('cuda')\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load(PATH))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22521c35",
   "metadata": {},
   "source": [
    "- `my_tensor.to(device)`를 호출하면 GPU에 `my_tensor`의 복사본이 반환되며, 이는 `my_tensor`를 덮어쓰는 것이 아님\n",
    "- 그러므로 Tensor를 직접 덮어써 주어야 한다는 것을 기억하기 : `my_tensor = my_tensor.to(torch.device('cuda'))`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326b5023",
   "metadata": {},
   "source": [
    "## CPU에서 저장하고 GPU에서 불러오기\n",
    "- CPU에서 학습하고 저장된 모델을 GPU에서 불러올 때는 `torch.load()`함수의 `map_location`인자를 `cuda:device_id`로 설정해주기\n",
    "- 그러면 주어진 GPU 장치에 모델이 불러와짐\n",
    "- 모델의 매개변수 Tensor를 CUDA Tensor로 변환하기 위해, `model.to(torch.device(‘cuda’))`를 호출해주기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143daf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장하기\n",
    "torch.save(net.state_dict(), PATH)\n",
    "\n",
    "# 불러오기\n",
    "device = torch.device(\"cuda\")\n",
    "model = Net()\n",
    "\n",
    "# 사용하고자 하는 GPU 장치 번호를 지정합니다.\n",
    "model.load_state_dict(torch.load(PATH, map_location=\"cuda:0\"))\n",
    "\n",
    "# 모델에 사용되는 모든 입력 Tensor들에 대해 input = input.to(device) 을 호출해야 합니다.\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8965ef4e",
   "metadata": {},
   "source": [
    "## `torch.nn.DataParallel` 모델을 저장하고 불러오기\n",
    "- `torch.nn.DataParallel`은 병렬 GPU 활용을 가능하게 하는 모델 래퍼(wrapper)임\n",
    "- DataParallel 모델을 범용적으로 저장하기 위해서는 `model.module.state_dict()`를 사용하면 됨\n",
    "- 그러면 원하는 장치에 원하는 방식으로 유연하게 모델을 불러올 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fd8011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장하기\n",
    "torch.save(net.module.state_dict(), PATH)\n",
    "\n",
    "# 사용할 장치에 불러오기"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
